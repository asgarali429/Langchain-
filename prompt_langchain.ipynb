{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL4FG8rZscGR36MQfsAUde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asgarali429/Langchain-/blob/main/prompt_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install langchain-huggingface\n",
        "!pip install streamlit\n",
        "!pip install python-dotenv\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "yRohrdkPE2D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load Hugging Face API token securely from Colab userdata\n",
        "hf_api_token = userdata.get('huggingfacehub_api_token')\n",
        "\n",
        "# Configure the HuggingFace LLM using Mistral-7B\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=hf_api_token\n",
        ")\n",
        "\n",
        "# Wrap the LLM into a chat-friendly interface\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# Test basic interaction with the model\n",
        "result = model.invoke(\"What is the capital of India?\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "gEq4vhu5GGf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the summary prompt template to a file\n",
        "%%writefile template_generator.py\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Define a detailed and structured prompt for summarizing research papers\n",
        "template = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
        "Explanation Style: {style_input}\n",
        "Explanation Length: {length_input}\n",
        "1. Mathematical Details:\n",
        "   - Include relevant mathematical equations if present in the paper.\n",
        "   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.\n",
        "2. Analogies:\n",
        "   - Use relatable analogies to simplify complex ideas.\n",
        "If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.\n",
        "Ensure the summary is clear, accurate, and aligned with the provided style and length.\n",
        "\"\"\",\n",
        "    input_variables=['paper_input', 'style_input', 'length_input'],\n",
        "    validate_template=True\n",
        ")\n",
        "\n",
        "# Save prompt to file\n",
        "template.save(\"template.json\")"
      ],
      "metadata": {
        "id": "sYvpBJdeGNo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Hugging Face token in a .env file for secure use in Streamlit\n",
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(f\"HUGGINGFACEHUB_API_TOKEN={hf_api_token}\")"
      ],
      "metadata": {
        "id": "V4ynBWOnGSEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit application: Research Paper Summary Generator\n",
        "%%writefile app.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import streamlit as st\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Load API token from .env file\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "# Configure the LLM\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=api_key\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# Streamlit UI elements\n",
        "st.header(\"Research Tool\")\n",
        "\n",
        "# Dropdowns for user input\n",
        "paper_input = st.selectbox(\"Select Research Paper Name\", [\n",
        "    \"Attention Is All You Need\",\n",
        "    \"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
        "    \"GPT-3: Language Models are Few-Shot Learners\",\n",
        "    \"Diffusion Models Beat GANs on Image Synthesis\"\n",
        "])\n",
        "style_input = st.selectbox(\"Select Explanation Style\", [\n",
        "    \"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"\n",
        "])\n",
        "length_input = st.selectbox(\"Select Explanation Length\", [\n",
        "    \"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"\n",
        "])\n",
        "\n",
        "# Prompt template for generating the summary\n",
        "template = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
        "Explanation Style: {style_input}\n",
        "Explanation Length: {length_input}\n",
        "1. Mathematical Details:\n",
        "   - Include relevant mathematical equations if present in the paper.\n",
        "   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.\n",
        "2. Analogies:\n",
        "   - Use relatable analogies to simplify complex ideas.\n",
        "If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.\n",
        "Ensure the summary is clear, accurate, and aligned with the provided style and length.\n",
        "\"\"\",\n",
        "    input_variables=['paper_input', 'style_input', 'length_input'],\n",
        "    validate_template=True\n",
        ")\n",
        "\n",
        "# Generate and display the summary\n",
        "if st.button(\"Generate Summary\"):\n",
        "    chain = template | model\n",
        "    result = chain.invoke({\n",
        "        'paper_input': paper_input,\n",
        "        'style_input': style_input,\n",
        "        'length_input': length_input\n",
        "    })\n",
        "    st.write(result.content)"
      ],
      "metadata": {
        "id": "uCGNG0O0GXBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill existing Streamlit and ngrok processes if running\n",
        "!pkill -f streamlit\n",
        "!pkill -f ngrok\n",
        "\n",
        "# Authenticate ngrok\n",
        "!ngrok authtoken \"Your ngrok token\"  # Replace with your actual token"
      ],
      "metadata": {
        "id": "lp2xkwYZGZtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch Streamlit app and expose it using ngrok\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "# Run Streamlit app in the background\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "# Create a public URL using ngrok\n",
        "url = ngrok.connect(8501).public_url\n",
        "print(f\"Streamlit app running at: {url}\")"
      ],
      "metadata": {
        "id": "IkPUXlsnGh3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic chatbot using LangChain prompt template and Mistral-7B\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve API token\n",
        "hf_api_token = userdata.get('huggingfacehub_api_token')\n",
        "\n",
        "# Setup the LLM\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=hf_api_token\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# Create a chat template with memory\n",
        "chat_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "# Start chat loop\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    prompt = chat_template.invoke({'chat_history': chat_history, 'input': user_input})\n",
        "    result = model.invoke(prompt)\n",
        "\n",
        "    print(result.content)\n",
        "\n",
        "    # Update chat history\n",
        "    chat_history.append(HumanMessage(content=user_input))\n",
        "    chat_history.append(AIMessage(content=result.content))"
      ],
      "metadata": {
        "id": "N9KYdZRTGlV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}